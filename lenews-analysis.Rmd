---
title: "LeNews Analsiys"
output: html_notebook
---

This is an analysis of LeFloid News data

```{r}
mydata <- read.csv("lenews.csv")
mydata
```

# Quellentransparenz

Welche Quellen werden angegeben? Mit einer sehr simplen regulÃ¤ren Ausdruck lassen sich die URLs aus dem Description-Text extrahieren.

```{r}
#library(tidyr)
#library(dplyr)

pattern <- "(http[^[:space:]]+)"
m <- gregexpr(pattern, mydata$description)
urls <- regmatches(mydata$description, m)

#count_urls <- lapply(urls,length)

unique_urls <- unique(unlist(urls))

URL_parts <- function(x) {
    m <- regexec("^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)", x)
    parts <- do.call(rbind,
                     lapply(regmatches(x, m), `[`, c(3L, 4L, 6L, 7L)))
    colnames(parts) <- c("protocol","host","port","path")
    parts
}

URL_parts(unique_urls)

```



```{r}
library(ggplot2)

ggplot(data = mydata) + 
  geom_point(mapping = aes(x = id, y = duration)) +
  labs(title = "Like_count vs. lenght") + 
  scale_x_discrete(breaks=NULL)
```

```{r}
ggplot(data = mydata, aes(x = like_count, y = dislike_count)) + 
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = "Like vs. dislike in LeNews")
```
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
